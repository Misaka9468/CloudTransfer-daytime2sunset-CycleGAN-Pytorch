# CloudTransfer-daytime2sunset-CycleGAN-Pytorch

æœ¬é¡¹ç›®åŸºäº`Pytorch1.7.1`æ¡†æ¶ï¼Œå®ç°äº†CycleGANæ¨¡å‹ï¼Œä¸ç›¸åº”çš„è®­ç»ƒã€æµ‹è¯•ã€åº”ç”¨éƒ¨åˆ†åŠŸèƒ½.  

æˆåŠŸç”¨äº**ç™½äº‘<->æ™šéœ**ä¸**ğŸè‹¹æœ<->ğŸŠæ©˜å­**å›¾åƒé£æ ¼è½¬åŒ–ã€‚  
  
ç›¸è¾ƒäºåŸè®ºæ–‡ä»£ç ï¼Œæœ¬é¡¹ç›®çš„ä»£ç æ›´ç®€æ´ã€æ˜“è¯»ï¼Œä¸ªäººè®¤ä¸ºæ¯”è¾ƒé€‚åˆå…¥é—¨è€…åˆ©ç”¨CycleGANç½‘ç»œæ¨¡å‹å°è¯•å›¾åƒé£æ ¼è¿ç§»ã€‚  

**è®ºæ–‡åœ°å€ï¼š** https://arxiv.org/abs/1703.10593  
  
**éƒ¨åˆ†ä»£ç å‚è€ƒï¼š** https://github.com/aitorzip/PyTorch-CycleGAN  (ä½¿ç”¨æ¡†æ¶ä¸º`Pytorch 0.3`ï¼Œéƒ¨åˆ†ä»£ç åœ¨é«˜ç‰ˆæœ¬Pytorchä¸­å·²ç»å¤±æ•ˆ)
<br/><br/><br/>
This project is based on `Pytorch1.7.1`, implements CycleGAN model, and the corresponding training, testing, and application functions.  
  
Successfully used in the image style transfer of **Cloud: daytime<->sunset** and **ğŸapple<->ğŸŠorange**.  

Compared with the original paper code, the code of this project is more easy to read. I personally think it is more suitable for beginners to use the CycleGAN network model to try image style transfer.

**Paper:** https://arxiv.org/abs/1703.10593  

**Part of the code reference:** https://github.com/aitorzip/PyTorch-CycleGAN    
(Based on `Pytorch 0.3`, part of the code is invalid in the higher version of Pytorch)  

## ç»“æœå±•ç¤º(transfer result)

### ç™½äº‘<->æ™šéœ(cloud-daytime2sunset)

### è‹¹æœ<->æ©˜å­(apple2orange)

## è¿è¡Œç¯å¢ƒåŠéƒ¨åˆ†å®‰è£…åŒ…(Prerequisites)

* Python 3.8
* Pytorch(CUDA) 1.7.1
* torchvision 0.8.2
* numpy 1.20.2

* tensorboard 2.5.0 
  * ç”¨äºlossæ›²çº¿çš„ç»˜åˆ¶ä»¥åŠå¯è§†åŒ–ç”Ÿæˆå›¾ç‰‡
  * Used for drawing loss curve and visualization of generated pictures

## ä½¿ç”¨æŒ‡å—(Guide)

### è®­ç»ƒ(Training)

#### 1.å‡†å¤‡æ•°æ®é›†(Prepare dataset)

* dataset recommendedï¼šhttps://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/

* prepare your datasetï¼š
  All the images should have 256\*256 px size with RGB channel
  The directory structure be like
  
```
    â”œâ”€â”€ datasets                   
    |   â”œâ”€â”€ <your_datasets_name>   # i.e. apple2orange         
    |   |   â”œâ”€â”€ trainA             # about 1000 images of apples
    |   |   â”œâ”€â”€ trainB             # about 1000 images of oranges
    |   |   â”œâ”€â”€ testA              # about 200 images of apples
    |   |   â””â”€â”€ testB              # about 200 images of oranges
```

#### 2.å¼€å§‹è®­ç»ƒ(Start training)

* Do not use Colabï¼š
    In your Python environmentï¼š(For example)
    ```
    python train.py --root datasets/<your_dataset_name>/ --cuda
    ```
    
    `--cuda` is optional, you can use GPU to accelerate your training speed.   
    
    You can also specify other arguments such as `--size 256` `--lr 0.0002`. Check out train.py for details.  
    
    If you want to view the loss function graph and the generated pictures, you can use **tensorboard**.
    ```
    tensorboard --logdir = logs
    ```
    Then you can click the URL http://localhost:6006/ and check the result.   
    
    During the training, checkpoints will be saved in folder `checkpoints`,

* Use Colab:
  []

### æµ‹è¯•(Testing)

In your Python environmentï¼š(For example)
```
python test.py --root datasets/<your_dataset_name>/ --model_root trained_model/<your_dataset_name>/model.pth --cuda
```
`--cuda` is optional. 

**model.pth** is the pretrained model which contains 2 generators' parameters.  

Test result will be saved in folders: **test_output/outputA** and **test_output/outputB**

### åº”ç”¨(Apply)

Put the images you want to transfer into folder **apply/inA** or **apply/inB** (It depends on whether your images belongs to domain A or domain B.)  

In your Python enviroment: (For example)

```
python transfer.py --model_root trained_model/<your_dataset_name> --cuda
```

`--cuda` is optional.  

The result will be saved in folders: **apply/outputA** and **apply/outputB**  

**Notice:** All the results will be resized to 256\*256 px size. 





















